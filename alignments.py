import os
import xml.etree.ElementTree as ET
import re
from tqdm import tqdm


def split_align_file_docs(sent_align_path, word_align_path, split_dir):
    """
    Split the word alignment file into smaller files 
    each corresponding to one europarl file.

    Parameters
    ----------
    sent_align_path : str
        Xml file containing sentence alignments of the corpora.
    word_align_path : str
        File containing the word alignments for every sentence pair.
    split_dir : str
        Directory to save split files to.
    """

    tree = ET.parse(sent_align_path)
    docs = tree.iter(tag="linkGrp")
    with open(word_align_path) as word_align_file:
        #align_lines = word_align_file.readlines()
        for doc in tqdm(docs):
            fn = doc.attrib["fromDoc"][3:-7]
            links = list(doc.iter(tag="link"))
            num_sents = len(links)
            #curr_lines = align_lines[:num_sents]
            out_path = os.path.join(split_dir, fn)
            with open(out_path, "w") as out_file:
                for i, line in enumerate(word_align_file):
                    out_file.write(line)
                    if i == num_sents - 1:
                        break

#split_align_file_docs("/home/johann/Studium/IM/data/Europarl/smt/de-en/bitext.xml",
#                      "/home/johann/Studium/IM/data/Europarl/smt/de-en/model/aligned.intersection",
#                      "/home/johann/Studium/IM/data/europarl/alignments/en_full_intersection")

def intersection_alignment(src2tgt_path, tgt2src_path, intersection_path):
    """
    Get intersection of word alignments for two directions.

    Parameters
    ----------
    src2tgt_path : str
        Alignment file in one direction.
    tgt2src_path : str
        Alignment file in other direction.
    intersection_path : str
        File to save intersection to.
    """

    with open(src2tgt_path) as s2t_file, \
            open(tgt2src_path) as t2s_file, \
            open(intersection_path, "w") as int_file:
        for s2t_line, t2s_line in zip(s2t_file, t2s_file):
            s2t = s2t_line.split()
            s2t = [a.split("-") for a in s2t]
            s2t = [(int(i),int(j)) for (i,j) in s2t]

            t2s = t2s_line.split()
            t2s = [a.split("-") for a in t2s]
            t2s = [(int(i),int(j)) for (i,j) in t2s]

            inter = [tup for tup in s2t if tup in t2s]
            inter = [str(i) + "-" + str(j) for (i,j) in inter]
            inter = " ".join(inter)
            int_file.write(inter)
            int_file.write("\n")


#intersection_alignment("/home/johann/Studium/IM/data/Europarl/smt/de-en/model/aligned.srctotgt", 
#        "/home/johann/Studium/IM/data/Europarl/smt/de-en/model/aligned.tgttosrc", 
#        "/home/johann/Studium/IM/data/Europarl/smt/de-en/model/aligned.intersection")


def split_aligned(aligned_path, comb1_path, comb2_path):
    """
    Split the aligned file generated by Gale-Church
    using the implementation from github.com/alvations/gachalign/.
    Save the aligned sentences to one file per language.

    Parameters
    ----------
    aligned_path : str
        Path of file containing aligned sentences for both languages.
    comb1_path : str
        Path to save sentences from first language to.
    comb2_path : str
        Path to save sentences from second language to.
    """
    fn_line = "tmp"
    
    with open(comb1_path, "w") as comb1, open(comb2_path, "w") as comb2:
        prev_line_hash = False
        for line in open(aligned_path):
            if line[0] == "#" and not prev_line_hash:
                comb1.write(line)
                comb2.write(line)
                prev_line_hash = True
            elif line[0] == "#" and prev_line_hash:
                continue
            else:
                line = line[:-1]
                l1, l2 = line.split("\t")
                comb1.write(l1)
                comb1.write("\n")
                comb2.write(l2)
                comb2.write("\n")
                prev_line_hash = False
#split_aligned("/data/europarl/common/sent_aligned/de_en.txt",
#              "/data/europarl/common/sent_aligned/de_en4/de_comb.txt",
#              "/data/europarl/common/sent_aligned/de_en4/en_comb.txt")

def rm_dok_names(comb_path, inds_path, out_path):
    """
    Take a combined file containing many europarl speeches.
    Remove the lines indicating a new speech,
    i.e. the ones beginning with "##############################".
    Also remove blank lines.
    For each speech, write the filename and the number
    of lines into a new file.

    Parameters
    ----------
    comb_path : str
        Original file containing speeches.
    inds_path : str
        File to save filenames and numbers of lines to.
    out_path : str
        File to save the speeches to.
    """

    with open(comb_path) as orig_file:
        with open(inds_path, "w") as inds_file, \
                open(out_path, "w") as out_file:
            curr_fn = None
            curr_num = 0
            for line in orig_file:
                #if line.strip() == "":
                #    continue
                if line[0] == "#":
                    if curr_fn is not None and curr_num != 0:
                        inds_file.write(curr_fn + " " + str(curr_num) + "\n")
                    curr_fn = line.split("#")[-1][:-1]
                    curr_num = 0
                else:
                    curr_num += 1
                    out_file.write(line)

#rm_dok_names("/data/europarl/common/sent_aligned/de_en4/en_comb.txt", 
#             "/data/europarl/common/sent_aligned/de_en4/en_sent_inds.txt",
#             "/data/europarl/common/sent_aligned/de_en4/en_no_fn.txt")
#rm_dok_names("/data/europarl/common/sent_aligned/de_en4/de_comb.txt", 
#             "/data/europarl/common/sent_aligned/de_en4/de_sent_inds.txt",
#             "/data/europarl/common/sent_aligned/de_en4/de_no_fn.txt")


def split_giza_results(giza_dir, sent_inds_path, out_dir):
    """
    Split the results of the giza run according to the europarl files.

    Parameters
    ----------
    giza_dir : str
        Path to directory containing giza output files.
    sent_inds_path : str
        File containing the europarl filenames and the
        corresponding number of sentence pairs.
    out_dir : str
        Path to write the resulting files to.
    """
    fns = os.listdir(giza_dir)
    fns = [fn for fn in fns if fn[:15]=="src_trg.dict.A3"]

    #sent_pairs = dict()
    alg_lines = dict()

    num_reg = r"\(([0-9]+)\)"
    for fn in fns:
        giza_path = os.path.join(giza_dir, fn)
        line_ind = 0
        with open(giza_path) as f:
            #lines = f.readlines()
            #while lines != []:
            for line in f:
                #curr_lines = lines[:3]
                #lines = lines[3:]
                if line_ind == 0:
                    # line containing information about sentence pair
                    #match = re.search(num_reg, curr_lines[0])
                    match = re.search(num_reg, line)
                    num = int(match.group(1))
                    line_ind = 1
                elif line_ind == 1:
                    # line containing target language text
                    alg_lines[num] = line[:-1]
                    line_ind = 2
                else:
                    alg_lines[num] = (alg_lines[num], line[:-1])
                    line_ind = 0
                
                #line_pair = (curr_lines[1], curr_lines[2])
                #alg_lines[num] = curr_lines[2]
                #sent_pairs[num] = line_pair

    sent_count = 1
    with open(sent_inds_path) as sent_inds:
        for j, line in enumerate(sent_inds):
            fn, num = line.split()
            num = int(num)

            wc_src = 0
            wc_trg = 0

            out_path = os.path.join(out_dir, fn)
            with open(out_path, "w") as out_file:
                for i in range(num):
                    line_trg, line_src = alg_lines[sent_count + i]
                    word_alg = line_src.split(" })")
                    if word_alg[-1].strip() == "":
                        word_alg = word_alg[:-1]
                    word_alg = [s.split(" ({") for s in word_alg]
                    alg_line = ""
                    for i, (word, inds) in enumerate(word_alg):
                        if i == 0:
                            #ignore NULL word
                            continue
                        inds = inds.split()
                        for ind in inds:
                            ind_trg = str(wc_trg + int(ind))
                            ind_src = str(wc_src + i)
                            alg_line += ind_src + "-" + ind_trg + " "
                    alg_line += "\n"
                    out_file.write(alg_line)

                    wc_src += len(word_alg) - 1
                    wc_trg += len(line_trg.strip().split())

            sent_count += num

split_giza_results("/data/europarl/common/sent_aligned/en_de_def_dist/results/",
                   "/data/europarl/common/sent_aligned/de_en_defdist/de_sent_inds.txt",
                   "/data/europarl/common/word_aligned/en_de2/")
split_giza_results("/data/europarl/common/sent_aligned/de_en_defdist/results/",
                   "/data/europarl/common/sent_aligned/de_en_defdist/de_sent_inds.txt",
                   "/data/europarl/common/word_aligned/de_en2/")
                   #"/data/europarl/common/test2/")
